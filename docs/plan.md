PKMS â€“ System Baseline (v0.3, 2025-11-14)

Ziel: Ein versioniertes, agentenkompatibles Wissenssystem mit

Markdown-Dateien als Source of Truth,

JSON-Metadaten als strukturierte Wahrheit,

Typesense als Such-/Indexschicht,

deterministischem Chunking + Embeddings,

Konsolidierung & Vergessen (Wissensstoffwechsel),

und voller Reproduzierbarkeit via Git.



---

1. Systemstruktur

ğŸ“ Verzeichnisaufbau

pkms/
â”œâ”€â”€ notes/                      # Markdown-Quellen (SoT)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ records/                # JSON-Metadaten (1:1 zu notes/)
â”‚   â”œâ”€â”€ chunks/                 # NDJSON pro Dokument (1 Zeile = 1 Chunk)
â”‚   â””â”€â”€ embeddings/             # Embedding-Vektoren (pro Modell + Chunk-Hash)
â”‚       â”œâ”€â”€ text-3-large-2025-06/    # {chunk_hash}.npy
â”‚       â””â”€â”€ clip-vit-large/          # optional: Bild-Embeddings
â”œâ”€â”€ taxonomy/                   # kontrollierte Vokabulare
â”œâ”€â”€ schema/                     # JSON-Schemas (â†’ Pydantic Code-Gen)
â”œâ”€â”€ pkms/                       # Core-Library & Tools
â”‚   â”œâ”€â”€ lib/
â”‚   â””â”€â”€ tools/
â”œâ”€â”€ docker/                     # Deployment (Typesense etc.)
â”œâ”€â”€ policy/                     # Relevance- & Memory-Regeln
â”œâ”€â”€ Makefile
â””â”€â”€ .env

ğŸ§  Scripts (Toolchain)

Script	Aufgabe

ingest.py	Markdown â†’ JSON (Records), Auto-Detection Language/Dates
link.py	Wikilinks erkennen, validieren & bidirektional tracken
chunk.py	Hybrid Chunking (hierarchisch + semantisch), Content-Hash IDs
embed.py	Incremental Embedding (nur neue/geÃ¤nderte Chunks)
index_rebuild.py	Reindex in Typesense (Blue-Green)
search.py	Hybrid Search (BM25 + ANN via Typesense), MMR + Exploration
relevance.py	Formel-basiertes Scoring (Recency + Links + Quality + User)
synth.py	Konsolidierung via Git-Branches (Agent â†’ PR â†’ Merge)
archive.py	Policy-gesteuertes Archivieren (ohne LÃ¶schen)



---

2. Datenstrukturen

ğŸ§¾ Record (Markdown-Metadaten)

{
  "id": "01HAR6DP2M7G1KQ3Y3VQ8C0Q",
  "slug": "pizza-knusprig",
  "path": "notes/pizza-knusprig--01HAR6DP2M7G1KQ3Y3VQ8C0Q.md",
  "title": "Pizza â€“ knusprig bei 300Â°C",
  "tags": ["kochen", "ofen"],
  "categories": ["Kochen"],
  "language": "de",  // aus Frontmatter (auto-detect, Ã¼berschreibbar)
  "created": "2025-01-10T14:22:31Z",
  "updated": "2025-11-13T09:03:00Z",
  "date_semantic": "2025-01-09T18:00:00Z",  // aus Frontmatter
  "full_text": "...",
  "links": [
    {"raw": "[[kochen]]", "type": "slug", "target": "01HAR999", "resolved": true}
  ],
  "backlinks": [  // Bidirektional!
    {"from": "01HAR888", "context": "...siehe [[pizza-knusprig]]..."}
  ],
  "content_hash": "sha256:...",
  "file_hash": "sha256:...",
  "status": {
    "relevance_score": 0.82,
    "archived": false,
    "consolidated_into": null  // ULID der Synthese, falls konsolidiert
  },
  "agent": {"id": "linker", "confidence": 0.94, "reviewed": false},
  "embedding_meta": {
    "text": {
      "model": "text-3-large-2025-06",
      "dim": 3072,
      "updated_at": "2025-11-14T08:10:01Z",
      "chunk_hashes": ["a3f2bc1d", "f9e1a2b3", "c4d5e6f7"]  // Welche Chunks embedded
    }
  },
  "source": {"repo": "github.com/you/pkms", "commit": "84acâ€¦f2"}
}

ğŸ§© Chunk (Textabschnitt)

{
  "doc_id": "01HAR6DP2M7G1KQ3Y3VQ8C0Q",
  "chunk_id": "01HAR6DP2M7G1KQ3Y3VQ8C0Q:a3f2bc1d",
  "chunk_hash": "a3f2bc1d",  // xxhash64(text)[:12] â†’ Content-addressable
  "chunk_index": 7,
  "text": "...",
  "tokens": 472,
  "section": "Ofentemperatur",
  "language": "de"
}

ğŸ”¢ Embedding (Filesystem-basiert)

# Storage: data/embeddings/{model}/{chunk_hash}.npy
# Beispiel: data/embeddings/text-3-large-2025-06/a3f2bc1d.npy

# File enthÃ¤lt nur: np.array([0.0123, -0.9876, ...], dtype=float32)
# Metadaten stehen im Record (embedding_meta) und Chunk (chunk_hash)
# Timestamp = File mtime

# Vorteile:
# - Content-addressable: gleicher Text â†’ gleiche Datei
# - Deduplication Ã¼ber Docs hinweg
# - Incremental Updates: nur neue Hashes embedden
# - Modellwechsel: neuer Ordner, alte bleiben


---

2.1 Frontmatter vs JSON â€“ Daten-Philosophie

**Frontmatter (Markdown-Header):**
- âœ… Manuell editierbare Felder
- âœ… LLM-generiert, aber Ã¼berschreibbar
- âœ… FÃ¼r Menschen + Obsidian/Markdown-Tools lesbar

**Beispiel:**
```yaml
---
title: Pizza perfekt
tags: [kochen, italienisch]
categories: [Rezepte]
language: de  # Auto-detected, aber Ã¼berschreibbar
date_semantic: 2025-01-09  # "Wann war das Event?"
---
```

**JSON (data/records/):**
- âœ… Automatisch generierte Metadaten
- âœ… Nur fÃ¼r Tooling, nicht fÃ¼r manuelle Edits
- âœ… Abgeleitete Werte (Hashes, Links, Scores)

**Beispiel:**
```json
{
  "id": "01HAR6DP...",
  "slug": "pizza-perfekt",
  "created": "2025-11-14T10:22:31Z",  // File-Timestamp
  "updated": "2025-11-14T12:03:00Z",
  "content_hash": "sha256:...",
  "links": [...],  // Extrahiert aus [[wikilinks]]
  "backlinks": [...],  // Berechnet via link.py
  "status": {
    "relevance_score": 0.82,  // Berechnet via relevance.py
    "archived": false
  }
}
```

**Prinzip:**
> **Frontmatter** = Was Menschen/LLMs setzen sollen  
> **JSON** = Was Tools automatisch ableiten

**Workflow (ingest.py):**
1. Parse Frontmatter
2. Falls `language` fehlt â†’ Auto-Detect & zurÃ¼ckschreiben
3. Generiere JSON mit abgeleiteten Feldern
4. Synchron halten via File-Hashes


---

2.2 Bidirektionales Link-Tracking

**Forward Links (im Source-Doc):**
```json
{
  "id": "01HAR6DP...",
  "links": [
    {
      "raw": "[[kochen]]",
      "type": "slug",  // slug|id|alias
      "target": "01HAR999",  // Resolved ULID
      "resolved": true,
      "context": "...bei 300Â°C [[kochen]]..."
    }
  ]
}
```

**Backlinks (im Target-Doc):**
```json
{
  "id": "01HAR999",  // kochen-Doc
  "backlinks": [
    {
      "from": "01HAR6DP",  // pizza-Doc
      "raw": "[[kochen]]",
      "context": "...bei 300Â°C [[kochen]]..."
    }
  ]
}
```

**Workflow (link.py):**
1. Extrahiere alle `[[wikilinks]]` aus Markdown
2. Resolve via Slug/ID/Alias â†’ ULID
3. Schreibe `links` in Source-Record
4. Schreibe `backlinks` in Target-Record
5. Validierung: Warne bei broken links (target=null)

**Use Cases:**
- **Orphan Detection**: Docs ohne Backlinks
- **Graph Analysis**: PageRank fÃ¼r Relevance-Scoring
- **Cluster Detection**: Hohe Link-Dichte â†’ Synth-Kandidaten
- **Archive Warnings**: "Doc hat 12 Backlinks, wirklich archivieren?"

**Visualisierung:**
```python
# Generiere Graph: docs â†’ nodes, links â†’ edges
# Export: Graphviz DOT, D3.js JSON, Obsidian Graph
```


---

3. Betrieb und Konsistenz

ğŸŒ€ Blue-Green Deployment fÃ¼r Typesense

Indexiere in versionierte Collections: kb_chunks_v17, kb_docs_v17.

Suche lÃ¤uft Ã¼ber Alias: kb_chunks, kb_docs.

Neuer Index â†’ neue Version (_v18) â†’ Alias-Swap nach Abschluss.

RÃ¼cksprung per Alias, kein Downtime.


ğŸ§© Embedding-Migration & Modellwechsel

**Filesystem-basierte Migration:**
- Neues Modell â†’ neuer Ordner: `data/embeddings/text-4-large-2026-01/`
- Alte Embeddings bleiben: `data/embeddings/text-3-large-2025-06/`
- Record-JSON trackt aktuelles Modell in `embedding_meta.text.model`

**Migration-Workflow:**
```python
# embed.py --model text-4-large-2026-01 --migrate
# â†’ Re-embedded alle Chunks in neuen Ordner
# â†’ Updated alle Records: embedding_meta.text.model
# â†’ Alte .npy Files bleiben (Rollback-fÃ¤hig)
```

**Query-Zeit:**
- Policy wÃ¤hlt Modell: `search --embedding-model text-4-large-2026-01`
- Fallback auf altes Modell wenn neue Embeddings fehlen
- Typesense-Collection pro Modell (Blue-Green)

**Garbage Collection:**
```bash
# LÃ¶sche altes Modell nach erfolgreicher Migration
rm -rf data/embeddings/text-3-large-2025-06/
```


âš™ï¸ Relevanzsteuerung & SerendipitÃ¤t

Zeit-Decay mit Mindestschwelle (z.â€¯B. 0.15).

ZufÃ¤llige Exploration (Îµâ€‘Exploration, 5â€¯â€“â€¯10â€¯%).

DiversitÃ¤t durch MMR-Re-Ranking.

RegelmÃ¤ÃŸiger Zufalls-Recall alter, hochqualitativer EintrÃ¤ge.


ğŸ§± Chunking-QualitÃ¤t

Overlap: 10â€“20â€¯% zwischen Chunks.

Hierarchisch: Segmentierung nach Headings + LÃ¤nge.

Speichere section / subsection / page fÃ¼r Gruppierung.


ğŸ’¾ Embedding-Speicherstrategie

**Storage-Format:**
- `data/embeddings/{model}/{chunk_hash}.npy` (NumPy array, float32)
- Eine Datei pro Chunk-Hash (Content-addressable)
- Metadaten in Record-JSON (`embedding_meta`)

**GrÃ¶ÃŸe-Management:**

| GrÃ¶ÃŸe | Storage | Git-Strategie |
|-------|---------|---------------|
| Klein (<100MB) | `.npy` Files | Direkter Commit |
| Mittel (<1GB) | `.npy` Files | Git-LFS |
| GroÃŸ (>1GB) | Parquet + S3 | Commit-Pin in .env |

**Vorteile:**
- Incremental Updates: Nur neue Hashes embedden
- Deduplication: Gleicher Text in mehreren Docs = ein Embedding
- Modellwechsel: Neuer Ordner, alte bleiben
- Garbage Collection: LÃ¶sche `.npy` wenn kein Doc referenziert

**SoT-Hierarchie:**
- NDJSON (Chunks) = Quelle
- `.npy` Files (Embeddings) = Abgeleitete Artefakte
- Typesense = Cache


---

4. Wissensstoffwechsel: Konsolidierung & Vergessen

Ziel: Wissen bewertet, verdichtet und altert kontrolliert â€“ analog zu einem organischen GedÃ¤chtnis.

ğŸ¯ Kernmechanismen

Konsolidierung: Synth-Agent verdichtet redundante Notizen zu stabilen Synthesen.

Vergessen: Alte, irrelevante Daten werden archiviert oder deaktiviert.

Relevanzsteuerung: Nutzung, Vernetzung und Alter bestimmen Gewicht.


âš™ï¸ Prozesse

Relevance-Job (``): Score-Berechnung & Archivierung.

Synth-Agent (``): Themencluster erkennen, Synthesen generieren.

Archive-Skript (``): Policy-gesteuertes Altern & Verschieben.

Index-Gate: Nur archived:false & relevance_score>0.3 aktiv.


ğŸ§® Umsetzung

Schritt	Beschreibung

Schema-Erweiterung	status.*-Felder fÃ¼r Relevanz & Archivierung
Policy-Datei	/policy/memory.yaml fÃ¼r Relevance-Regeln
Synth-Agent	Konsolidierung thematisch verwandter Notes
Archive-Skript	Archivierung veralteter Records
Index-Gate	Filter auf aktive & relevante Inhalte


ğŸ’¡ Prinzipien

Bewahren statt LÃ¶schen: alles versioniert.

Transparenz: jede Entscheidung erzeugt Traces.

Determinismus: gleiche Eingabe â†’ gleiche Bewertung.

SerendipitÃ¤t: kontrollierte Zufallswiedervorlage.



---

5. Synth-Review-Prozess (Git-basiert)

ğŸ”„ Workflow

**1. Agent erstellt Feature-Branch**
```bash
git checkout -b synth/pizza-consolidated-01HAR789
```

**2. Agent schreibt Synthese + aktualisiert Quellen**
- Neue Note: `notes/pizza-perfekt--01HAR789.md`
- Neue Record: `data/records/pizza-perfekt--01HAR789.json`
- Update Quell-Records: `status.consolidated_into = "01HAR789"`
- Optional: `status.archived = true`

**3. Agent committed mit Trace**
```bash
git commit -m "synth: Consolidate 5 pizza recipes

Sources: 01HAR111, 01HAR222, 01HAR333
Synthesis: 01HAR789
Agent: synth-v1.2.0
Confidence: 0.89"
```

**4. Human Review**
- Via GitHub PR oder `git diff main..synth/pizza-consolidated-01HAR789`
- PrÃ¼ft: Inhalt, Links, Claim-Check, LÃ¤nge

**5a. Approved â†’ Merge**
```bash
git checkout main
git merge --no-ff synth/pizza-consolidated-01HAR789
```

**5b. Rejected â†’ Branch lÃ¶schen oder manuell fixen**
```bash
git branch -D synth/pizza-consolidated-01HAR789  # oder
git checkout synth/pizza-consolidated-01HAR789
# ... manual edits ...
git commit -m "human: Refined synthesis"
```

**6. Rollback**
```bash
git revert <merge-commit>
```

**Vorteile:**
- âœ… Locking via Git (keine parallelen Synths auf selben Docs)
- âœ… Review-UI vorhanden (GitHub/GitLab)
- âœ… Traces = Commit-History
- âœ… Kein Custom-Staging-Filesystem

âœ… Review-Checkliste



---

6. Wirkung

SignalstÃ¤rke: aktive, geprÃ¼fte Inhalte im Fokus.

Selbstreinigung: irrelevante Inhalte verblassen.

ErinnerungsfÃ¤higkeit: alte Ideen bleiben rekonstruierbar.

Kognitive Effizienz: Agents arbeiten auf kuratiertem Wissen.


> Das System erinnert, verdichtet, vergisst â€“ wie ein organisches GedÃ¤chtnis.




---

7. Facets & Mapping-Registry

Ziel: Einheitliche, kuratierte Facets im Index â€“ ohne das SoT aufzublÃ¤hen. Facets werden nur fÃ¼r real genutzte Filter/Aggregationen angelegt (niedrigeâ€“mittlere KardinalitÃ¤t).

7.1 Facet-Policy (global)

Immer facet: doc_id (grouping), doc_type, tags[], categories[], language, status.archived, agent.reviewed

Optional facet (falls nÃ¶tig): project, content_type, source.repo

Nicht facet: aliases[], freie Entities ohne Normalisierung, Pfade, Commits (hohe KardinalitÃ¤t)

Zeit-Felder: als int64 ms (filter/sort), keine Facets: updated_ms, date_semantic_ms


7.2 Type-spezifische Facets (flattened)

Im Record bleiben type-spezifische Felder namespaced unter facets.<type>.*. Der Index erhÃ¤lt eine abgeflachte Auswahl pro doc_type.

Beispiel Record (Ausschnitt):

{
  "doc_type": "invoice",
  "facets": {
    "invoice": { "amount": 1299.00, "currency": "EUR", "due_date": "2025-12-15" },
    "pdf": { "pages": 2, "author": "ACME Finance" }
  }
}

Mapping-Registry â†’ Index (Beispiel):

INDEX_FACET_REGISTRY = {
  "invoice": {
    "inv_amount": ("facets.invoice.amount", "float"),
    "inv_currency": ("facets.invoice.currency", "string"),
    "inv_due_ms": ("facets.invoice.due_date", "date_ms")
  },
  "pdf": {
    "pdf_pages": ("facets.pdf.pages", "int32"),
    "pdf_author": ("facets.pdf.author", "string")
  }
}

7.3 Chunk-Facets (multimodal)

Immer: doc_id (facet), modality (facet: text|caption|ocr|figure|table|asr|summary), language

Optional: section, subsection, page, t_start, t_end (fÃ¼r Filter/Range)



---

8. DBâ€‘Agnostik & Hybrid Search

**PrimÃ¤re Engine: Typesense**
- âœ… Hybrid Search (BM25 + Vector) in einem Query
- âœ… Native Grouping (`group_by=doc_id`, max 3 Chunks pro Doc)
- âœ… Faceting & Filtering out-of-the-box

**Prinzip: Engine-Agnostik**
- SoT bleibt Dateisystem (JSON/NDJSON)
- Such-/Vektor-Layer austauschbar via dÃ¼nne Abstraktion
- Alternative Engines: Meilisearch, Qdrant, pgvector

8.1 Such-Interface

class SearchIndex:
    def upsert_docs(self, docs: list[dict]) -> None: ...
    def delete_docs(self, ids: list[str]) -> None: ...

    def bm25(self, q: str, filters: dict | None, top_k: int) -> list[dict]: ...
    def ann(self, vector: list[float], filters: dict | None, top_k: int, model: str | None = None) -> list[dict]: ...
    def hybrid(self, q: str, vector: list[float] | None, filters: dict | None, top_k: int) -> list[dict]: ...

Implementierungen:

index_typesense.py â†’ BM25 + optional eingebaute Vektor-Suche, Grouping via group_by=doc_id

index_qdrant.py â†’ reine ANN (payload-Filter), fÃ¼r Fusion genutzt

index_pgvector.py â†’ SQLâ€‘basiertes ANN fÃ¼r kleineâ€“mittlere Skalen


8.2 Hybrid-Fusion (engineâ€‘neutral)

Hole k1 BM25 (Typesense/Meili), k2 ANN (Qdrant/pgvector/Typesense)

Scoreâ€‘Fusion (RRF oder gewichtete Summe), MMR fÃ¼r DiversitÃ¤t

Grouping: immer doc_id â†’ max. N Chunks pro Dokument


Pseudocode (RRF):

from collections import defaultdict

def rrf_merge(bm25_hits, ann_hits, k=60):
    S = defaultdict(float)
    for r, h in enumerate(bm25_hits, 1): S[h["doc_id"]] += 1/(k+r)
    for r, h in enumerate(ann_hits, 1):  S[h["doc_id"]] += 1/(r)
    return sorted(S.items(), key=lambda x: -x[1])

8.3 Blueâ€‘Green & Aliases (engineâ€‘agnostisch)

Collections versionieren: kb_chunks_vN, kb_docs_vN

Aliases: kb_chunks, kb_docs â†’ atomischer Swap nach Rebuild

Gilt identisch fÃ¼r Typesense, Meili, Qdrant (Collections/Aliases) und kann bei pgvector via Views/Schemata emuliert werden


8.4 Embeddingâ€‘Schienen

Textâ€‘Space (Pflicht) + optionale Image/Audioâ€‘Spaces

Felder pro Space: embedding_<space>, embedding_model_<space>, embedding_dim_<space>, embedding_at_<space>

Query wÃ¤hlt Space explizit; Fusion kombiniert Spaces (optional)



---

9. Aktualisierte Schemaâ€‘Ausschnitte (mit Facets & Multimodal)

9.1 Record (ergÃ¤nzt)

{
  "id": "01HAR6...",
  "doc_type": "note",
  "tags": ["kochen", "ofen"],
  "categories": ["Kochen"],
  "language": "de",
  "date_semantic": "2025-01-09T18:00:00Z",
  "status": { "relevance_score": 0.8, "archived": false },
  "agent": { "id": "linker", "confidence": 0.94, "reviewed": false },
  "facets": { "pdf": { "pages": 2 } },
  "source": { "repo": "github.com/you/pkms", "commit": "84ac..." }
}

9.2 Chunk (ergÃ¤nzt)

{
  "doc_id": "01HAR6...",
  "chunk_id": "01HAR6...:a3f2bc1d",
  "chunk_hash": "a3f2bc1d",  // xxhash64(text)[:12]
  "chunk_index": 7,
  "modality": "caption",  // text|caption|ocr|figure|table|asr|summary
  "section": "Ergebnisse",
  "language": "de",
  "text": "Abbildung zeigt ...",
  "tokens": 42
}

9.3 Embedding (Filesystem, multiâ€‘space)

# File: data/embeddings/text-3-large-2025-06/a3f2bc1d.npy
# Content: np.array([0.0123, -0.9876, ...], dtype=float32)

# Optional: FÃ¼r Image-Space
# File: data/embeddings/clip-vit-large/a3f2bc1d.npy

# Metadaten-Tracking im Record-JSON:
{
  "embedding_meta": {
    "text": {
      "model": "text-3-large-2025-06",
      "dim": 3072,
      "updated_at": "2025-11-14T08:10:01Z",
      "chunk_hashes": ["a3f2bc1d", "f9e1a2b3", ...]
    }
  }
}


---

10. Defaults fÃ¼r Indexâ€‘Schemas (Engineâ€‘neutral)

Docsâ€‘Collection (pro Dokument):

searchable: title, full_text

facets: doc_type, tags[], categories[], language, status.archived, agent.reviewed

filter/sort: date_semantic_ms, updated_ms, status.relevance_score


Chunksâ€‘Collection (pro Chunk):

searchable: text, title

facets: doc_id, modality, language, (optional) section, page

vectors: embedding_text: float[] (optional: embedding_image[], embedding_audio[])

sort: updated_ms


> Hinweis: Die tatsÃ¤chliche Felddefinition (Typesense/Meili/Qdrant/pgvector) wird in den jeweiligen Adapterâ€‘Modulen abgebildet. Das SoT bleibt engineâ€‘neutral.

---

11. Key Design Decisions (v0.3)

**Chunk-IDs: Content-Hash statt Index**
- âœ… `chunk_id = doc_id:xxhash64(text)[:12]`
- âœ… Deterministisch, dedup-fÃ¤hig, embedding-cache-freundlich
- âŒ Sequenzielle IDs (brechen bei Re-Chunking)

**Embeddings: Filesystem statt NDJSON**
- âœ… `data/embeddings/{model}/{chunk_hash}.npy`
- âœ… Incremental updates, modell-agnostisch, garbage-collectable
- âŒ Embeddings in Chunk-NDJSON (schlecht fÃ¼r Tracking)

**Synth-Review: Git-Branches statt Staging**
- âœ… Feature-Branch â†’ PR â†’ Merge
- âœ… Native Git-Locking, Rollback, Review-UI
- âŒ Custom `staging/` Filesystem

**Link-Tracking: Bidirektional**
- âœ… Forward Links + Backlinks
- âœ… Graph-Analyse, Orphan-Detection, Archive-Warnings
- âŒ Nur Forward Links (verliert Kontext)

**Relevance-Scoring: Formel statt Agent**
- âœ… Deterministisch, nachvollziehbar, A/B-testbar
- âœ… `0.4*recency + 0.3*links + 0.2*quality + 0.1*user`
- âŒ LLM-basiertes Scoring (teuer, non-deterministisch)

**Chunking: Hybrid (Hierarchisch + Semantisch)**
- âœ… Markdown-Headings + Sentence-Embedding-Splits
- âœ… Respektiert Struktur + semantische KohÃ¤renz
- âŒ Nur fix-size Chunks (ignoriert Semantik)

**Search: Typesense als Primary**
- âœ… Hybrid (BM25+Vector), Grouping, Faceting
- âœ… Engine-agnostisch via Abstraktion
- âŒ Meilisearch (kein natives Grouping)

**Frontmatter vs JSON: Separation of Concerns**
- âœ… Frontmatter = Manuell/LLM-editierbar
- âœ… JSON = Tool-generiert, abgeleitet
- âŒ Alles in Frontmatter (blÃ¤ht Markdown auf)

**Schema: JSON-Schema â†’ Pydantic**
- âœ… `schema/*.schema.json` â†’ Code-Gen
- âœ… Single Source of Truth
- âŒ Code-First (Schema divergiert)

**Language: Frontmatter (auto-detect, Ã¼berschreibbar)**
- âœ… In YAML-Header, damit manuell korrigierbar
- âœ… Auto-Detection via `langdetect` wenn leer
- âŒ Nur in JSON (nicht Ã¼berschreibbar)
